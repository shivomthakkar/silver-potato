{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c47755d-bb6d-4cf7-9cbe-c7f94fa3d5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3eaa4d0-43d2-4237-820f-66987c9c8c9b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('imdb_dataset.csv')\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a08780e-83eb-4564-9b68-c740ad3fc30d",
   "metadata": {},
   "source": [
    "## Cleaning the data\n",
    "\n",
    "Let's \n",
    "- Convert everything to lowercase\n",
    "- Remove HTML tags\n",
    "- Smarty remove punctuations\n",
    "- Remove stopwords (as they don't truly contribute to the sentiment of the sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e78471-ef13-42a2-aa6d-829e4f8d3735",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "punctuations = string.punctuation\n",
    "\n",
    "def clean_df(df):\n",
    "    df['review'] = df['review'].str.lower()\n",
    "    df['review'] = df['review'].str.replace('<[^>]*>', '', regex=True)\n",
    "    \n",
    "    df['review'] = df['review'].str.replace('(?<!\\d)[.,-](?!\\d)', ' ', regex=True)\n",
    "\n",
    "    for p in punctuations:\n",
    "        df['review'] = df['review'].str.replace(p, ' ')\n",
    "\n",
    "    df['review'] = df['review'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))\n",
    "    \n",
    "    return df\n",
    "\n",
    "df = clean_df(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3adc93-9bf8-4da6-b45b-ced1830dbf3a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Convert the reviews into a 2D list - a list of sentences (which is a list of words)\n",
    "sentences = df['review'].apply(lambda review: [w for w in review.split(' ') if w != '']).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7471e41f-7072-4890-9e37-f065e86dd495",
   "metadata": {},
   "source": [
    "## Embed, encode and pad the data\n",
    "\n",
    "I will use Word2Vec for creating embeddings, then pad them and encode the labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5e97c9-0eea-4df8-9dd5-e1ce6c6026c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Word2Vec embedding model and set it up\n",
    "from gensim.models import Word2Vec\n",
    "word2vec = Word2Vec(sentences, vector_size=100, window=5, min_count=10, workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b5aae4-30ed-4c49-a30d-68ca11cb1454",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec.wv.most_similar(positive='good')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f049210-4088-4263-9f53-e38ae2e2b5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "w2v_weights = word2vec.wv.vectors\n",
    "vocab_size, embedding_size = w2v_weights.shape\n",
    "\n",
    "print(\"Vocabulary Size: {} - Embedding Dim: {}\".format(vocab_size, embedding_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ce631c-0cbe-44bd-adea-c111e64495e8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# A helper function to visualize embeddings in a 2D space\n",
    "# Shivom - Find a way to reuse this in other projects\n",
    "def visualize_embeddings(w2v_model, n_samples=500):\n",
    "    # Sample random words from model dictionary\n",
    "    random_i = random.sample(range(vocab_size), n_samples)\n",
    "    random_w = [w2v_model.wv.index_to_key[i] for i in random_i]\n",
    "\n",
    "    # Generate Word2Vec embeddings of each word\n",
    "    word_vecs = np.array([w2v_model.wv.get_vector(w) for w in random_w])\n",
    "    \n",
    "    # Apply t-SNE to Word2Vec embeddings, reducing to 2 dims\n",
    "    tsne = TSNE()\n",
    "    tsne_e = tsne.fit_transform(word_vecs)\n",
    "    \n",
    "    # Plot t-SNE result\n",
    "    plt.figure(figsize=(32, 32))\n",
    "    plt.scatter(tsne_e[:, 0], tsne_e[:, 1], marker='o', c=range(len(random_w)), cmap=plt.get_cmap('Spectral'))\n",
    "    \n",
    "    for label, x, y, in zip(random_w, tsne_e[:, 0], tsne_e[:, 1]):\n",
    "        plt.annotate(label,\n",
    "                     xy=(x, y), xytext=(0, 15),\n",
    "                     textcoords='offset points', ha='right', va='bottom',\n",
    "                     bbox=dict(boxstyle='round, pad=0.2', fc='yellow', alpha=0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7830bcfe-56f1-4f14-8a0f-3ce654d465f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "visualize_embeddings(word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ae381a-8edc-4457-b502-610a663216fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's encode and pad the sentences with Word2Vec, and the sentiment with a label encoder\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Sequences will be padded or truncated to this length\n",
    "MAX_SEQUENCE_LENGTH = 200\n",
    "\n",
    "def generate_sequence(df):\n",
    "    for idx, row in df.iterrows():\n",
    "        sentence = []\n",
    "        for w in row['review'].split(' ')[:MAX_SEQUENCE_LENGTH]:  # Shivom - Do I need to substring with the MAX_SEQUENCE_LENGTH?\n",
    "            if w != '' and w in word2vec.wv.key_to_index:\n",
    "                sentence.append(word2vec.wv.key_to_index[w])\n",
    "        yield np.array(sentence), row['sentiment']\n",
    "\n",
    "review_set = []\n",
    "sentiment_set = []\n",
    "\n",
    "for seq in generate_sequence(df):\n",
    "    review_set.append(seq[0])\n",
    "    sentiment_set.append(seq[1])\n",
    "\n",
    "review_set = pad_sequences(review_set, maxlen=MAX_SEQUENCE_LENGTH, padding='pre', value=0)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "sentiment_set = label_encoder.fit_transform(sentiment_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4681230-ebb7-405a-ad2e-b0b030d007f2",
   "metadata": {},
   "source": [
    "## Splitting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d71898b-e6fd-4f34-a9c5-bf7deb7e95db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Split the dataset into train and val\n",
    "\n",
    "import random\n",
    "\n",
    "TRAIN_TEST_SPLIT = 0.15\n",
    "\n",
    "total_samples = df.shape[0]\n",
    "n_val = int(TRAIN_TEST_SPLIT * total_samples)\n",
    "n_train = total_samples - n_val\n",
    "\n",
    "random_i = random.sample(range(total_samples), total_samples)\n",
    "train_x = review_set[random_i[:n_train]]\n",
    "train_y = sentiment_set[random_i[:n_train]]\n",
    "val_x = review_set[random_i[n_train: n_train + n_val]]\n",
    "val_y = sentiment_set[random_i[n_train: n_train + n_val]]\n",
    "\n",
    "print(\"Train Shapes - X: {} - Y: {}\".format(train_x.shape, train_y.shape))\n",
    "print(\"Val Shapes - X: {} - Y: {}\".format(val_x.shape, val_y.shape))\n",
    "\n",
    "# Let's look at the distribution of categories in both sets\n",
    "categories, ccount = np.unique(train_y, return_counts=True)\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.title(\"Training Set - Category Distribution\")\n",
    "plt.xticks(range(len(categories)), cat_dict.keys())\n",
    "plt.bar(categories, ccount, align='center')\n",
    "plt.show()\n",
    "\n",
    "categories, ccount = np.unique(val_y, return_counts=True)\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.title(\"Validation Set - Category Distribution\")\n",
    "plt.xticks(range(len(categories)), cat_dict.keys())\n",
    "plt.bar(categories, ccount, align='center')\n",
    "plt.show()\n",
    "\n",
    "n_categories = len(categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca65a0b-19d7-4140-83cb-197f1217035c",
   "metadata": {},
   "source": [
    "## Creating the model, compiling it and training\n",
    "\n",
    "- I will create a Sequential model with an Input layer, an Emedding Layer and a Dropout.\n",
    "- Next, we have a Bi-directional LSTM (studies show they work better than just LSTMs), \n",
    "- And then a Dense output layer with 1 neuron (output will be 0 or 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe04e7a-36d0-4965-917e-3b19d947961f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Bidirectional, Dense, Dropout, Embedding, Input, LSTM\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Input(shape=(MAX_SEQUENCE_LENGTH,)))\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_size, weights=[w2v_weights], mask_zero=True, trainable=False))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Bidirectional(LSTM(100, dropout=0.2, recurrent_dropout=0.2)))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c1728b-3af4-4bee-b0ec-d3849243efd9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's compile the model with the adam optimizer and the binary_crossentropy loss function because our output will be binary.\n",
    "# For metrics we will observe accuracy\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# history = model.fit(train_x, train_y, epochs=10, batch_size=32, validation_data=(val_x, val_y), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2fac90-2077-4810-be5f-aa772aa4effd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plotting Loss and Accuracy Graphs\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Loss')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Accuracy')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4f400f-d89b-4f98-9a83-fdc14b19b1f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's automate the hyperparameter tuning process and come back later\n",
    "epoch_options = [5, 10, 20]\n",
    "batch_sizes = [16, 32, 64, 128]\n",
    "tuning_results = []\n",
    "\n",
    "for epochs in epoch_options:\n",
    "    for batch_size in batch_sizes:\n",
    "        print(f\"Epochs: {epochs}, Batch Size: {batch_size}\")\n",
    "        history = model.fit(train_x, train_y, epochs=epochs, batch_size=batch_size, validation_data=(val_x, val_y), verbose=1)\n",
    "        tuning_results.append([epochs, batch_size, history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa1e661-0303-4e95-97be-7162ddd54223",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "res = []\n",
    "for history in tuning_results:\n",
    "    res.append([history[0], history[1], history[2].history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97954200-ec86-4f46-8b68-3f521e40c3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('res1.json', 'w') as f:\n",
    "    f.write(json.dumps(res))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a5fd6d-6dcd-4c05-866e-f644a8e28dc7",
   "metadata": {},
   "source": [
    "# Next steps\n",
    "\n",
    "1. Publish the results of the 'quick' optimization for-loops\n",
    "2. Look at hyperparameter optimization techniques\n",
    "    - Grid Search\n",
    "    - Random Search\n",
    "    - Bayesian Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968c1926-e46c-4177-b87b-eef2cc4d3a33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
